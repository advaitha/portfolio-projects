{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation Engines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Implicit Feedback\n",
    "+ Explicit Feedback\n",
    "+ Users should have given multiple ratings and service should have received multiple ratins from many different customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-personalized recommendations based on number of times interacted and average ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_df['book'].value_counts()\n",
    "print(book_df.value_counts().index) # Get the names of the books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of only movies appearing > 50 times in the dataset\n",
    "movie_popularity = user_ratings_df[\"title\"].value_counts()\n",
    "popular_movies = movie_popularity[movie_popularity > 50].index\n",
    "\n",
    "# Use this popular_movies list to filter the original DataFrame\n",
    "popular_movies_rankings =  user_ratings_df[user_ratings_df[\"title\"].isin(popular_movies)]\n",
    "\n",
    "# Find the average rating given to these frequently watched films\n",
    "popular_movies_average_rankings = popular_movies_rankings[[\"title\", \"rating\"]].groupby('title').mean()\n",
    "print(popular_movies_average_rankings.sort_values(by=\"rating\", ascending=False).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-personalized recommendations based on commonly occuring items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# permutationss (list, length_of_permutations) Generates iterable objects containing all permultations\n",
    "# list converts this object to a usable list\n",
    "#pd.DataFrame converts the list to a DataFrame\n",
    "\n",
    "from itertools import permutations\n",
    "\n",
    "def create_pairs(x):\n",
    "    pairs = pd.DataFrame(list(permutations(x.values, 2)), \n",
    "                         columns = ['book_a','book_b'])\n",
    "    return pairs    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the function to the data\n",
    "book_pairs = book_df.groupby('userid')['book_title'].apply(create_pairs)\n",
    "# Drop the index\n",
    "book_pairs = book_pairs.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting the pairings\n",
    "pair_counts = book_pairs.groupby['book_a','book_b'].size()\n",
    "\n",
    "# convert it into a dataframe\n",
    "pair_counts_df = pair_counts.to_frame(name = 'size').reset_index()\n",
    "\n",
    "# sort the values\n",
    "pair_counts_sorted = pair_counts_df.sort_values('size', ascending = False)\n",
    "\n",
    "# Filter for a book\n",
    "lord_of_rings = pair_counts_sorted[pair_counts_sorted['book_a'] == 'Lord of the Rings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "lord_of_rings.plot.bar(x = 'book_b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Content Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ As the recommendations are based on the item attributes rather than user feedback, recommendations can be made on never-before-purchased products\n",
    "+ The desired outcome is a row per movie with each column indicating whether a attribute applies to the item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the rows with values in the name column equal to Toy Story\n",
    "toy_story_genres = movie_genre_df[movie_genre_df['name'] == \"Toy Story\"]\n",
    "\n",
    "# Inspect the subset\n",
    "print(toy_story_genres)\n",
    "\n",
    "# Select only the rows with values in the name column equal to Toy Story\n",
    "toy_story_genres = movie_genre_df[movie_genre_df['name'] == 'Toy Story']\n",
    "\n",
    "# Create cross-tabulated DataFrame from name and genre_list columns\n",
    "movie_cross_table = pd.crosstab(movie_genre_df['name'], movie_genre_df['genre_list'])\n",
    "\n",
    "# Select only the rows with Toy Story as the index\n",
    "toy_story_genres_ct = movie_cross_table[movie_cross_table.index == 'Toy Story']\n",
    "print(toy_story_genres_ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jaccard Similarity\n",
    "+ The number of attributes that two items have in common (A intersection B) / The total number of their combined attributes (A U B)\n",
    "+ This values will be between 0 and 1. Higher the intersection, higher the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import jaccard_score\n",
    "hobbit_row = book_genre.df.loc['The Hobbit']\n",
    "GOT_row = book_genre_df.loc['A Game of Thrones']\n",
    "print(jaccard_score(hobbit_row, GOT_row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, squareform\n",
    "jaccard_distances = pdist(book_genre_df.values, metric = 'jaccard') # Create 1D array\n",
    "square_jaccard_distances = squareform(jaccard_distances)\n",
    "\n",
    "# As we want similarity we need to separate it from 1\n",
    "jaccard_similarity_array = 1 - square_jaccard_distances\n",
    "\n",
    "distance_df = pd.DataFrame(jaccard_similarity_array, \n",
    "                          index = genres_array_df['Book'],\n",
    "                          columns = genres_array_df['Book'])\n",
    "\n",
    "print(distance_df['The Hobbit']['A Game of Thrones'])\n",
    "print(distance_df['The Hobbit'].sort_values(ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import numpy and the distance metric\n",
    "import numpy as np\n",
    "from sklearn.metrics import jaccard_score\n",
    "\n",
    "# Extract just the rows containing GoldenEye and Toy Story\n",
    "goldeneye_values = movie_cross_table.loc['GoldenEye'].values\n",
    "toy_story_values = movie_cross_table.loc['Toy Story'].values\n",
    "\n",
    "# Find the similarity between GoldenEye and Toy Story\n",
    "print(jaccard_score(goldeneye_values, toy_story_values))\n",
    "\n",
    "# Repeat for GoldenEye and Skyfall\n",
    "skyfall_values = movie_cross_table.loc['Skyfall'].values\n",
    "print(jaccard_score(goldeneye_values, skyfall_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import functions from scipy\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "# Calculate all pairwise distances\n",
    "jaccard_distances = pdist(movie_cross_table.values, metric='jaccard')\n",
    "\n",
    "# Convert the distances to a square matrix\n",
    "jaccard_similarity_array = 1 - squareform(jaccard_distances)\n",
    "\n",
    "# Wrap the array in a pandas DataFrame\n",
    "jaccard_similarity_df = pd.DataFrame(jaccard_similarity_array, index=movie_cross_table.index, columns=movie_cross_table.index)\n",
    "\n",
    "# Print the top 5 rows of the DataFrame\n",
    "print(jaccard_similarity_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating features from Text for content Recommendation using tf-idf and cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skelearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidfvec = TfidfVectorizer(min_df = 2, max_df = 0.7) # features occured in atleast two documents, words occuring in more than 70% of the documents will be excluded\n",
    "vectorized_data = tfidfvec.fit_transform(book_summary_df['Descriptions'])\n",
    "print(tfidfvec.get_feature_names) # prints out the feature names that were generated\n",
    "print(vectorized_data.to_array()) #Generates a row for each book and a column for each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting to array to dataframe\n",
    "tfidf_df = pd.DataFrame(vectorized_data.toarray(), columns = tfidfvec.get_feature_names())\n",
    "tfidf_df.index = book_summary_df['Book']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cosine similarity\n",
    "cos(theta) = A.B / ||A||.||B|| # Intutively, the angle between the documents in high dimensional space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Finding similarity between all items\n",
    "cosine_similarity_array = cosine_similarity(tfidf_summary_df)\n",
    "\n",
    "# Finding similarity between two items\n",
    "cosine_similarity(tfidf_df.loc['The Hobbit'].values.reshape(1,-1),\n",
    "                 tfidf_df.loc['Macbeth'].values.reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import cosine_similarity measure\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Create the array of cosine similarity values\n",
    "cosine_similarity_array = cosine_similarity(tfidf_summary_df)\n",
    "\n",
    "# Wrap the array in a pandas DataFrame\n",
    "cosine_similarity_df = pd.DataFrame(cosine_similarity_array, index=tfidf_summary_df.index, columns=tfidf_summary_df.index)\n",
    "\n",
    "# Print the top 5 rows of the DataFrame\n",
    "print(cosine_similarity_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the preloaded array in a DataFrame\n",
    "cosine_similarity_df = pd.DataFrame(cosine_similarity_array, index=tfidf_summary_df.index, columns=tfidf_summary_df.index)\n",
    "\n",
    "# Find the values for the movie Thor\n",
    "cosine_similarity_series = cosine_similarity_df.loc['Rio']\n",
    "\n",
    "# Sort these values highest to lowest\n",
    "ordered_similarities = cosine_similarity_series.sort_values(ascending = False)\n",
    "\n",
    "# Print the results\n",
    "print(ordered_similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building user profiles based on the books ready by the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_books_read = ['The Hobbit', 'Foundation', 'Nudge']\n",
    "user_books = tfidf_summary_df.reindex(list_of_books_read) #reindex will subset the list of books from index\n",
    "user_prof = user_movies.mean() # mean of all the features for a particular user\n",
    "user_prof.values.reshape(1,-1) #Reshape the user_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding recommendations for a user\n",
    "non_user_movies = tfidf_summary_df.drop(list_of_movies_seen, axis = 0)\n",
    "\n",
    "# Finding the cosine similarity\n",
    "user_prof_similarities = cosine_similarity(user_prof.values.reshape(1, -1), non_user_movies)\n",
    "\n",
    "user_prof_similarities_df = pd.DataFrame(user_prof_similarities.T,\n",
    "                                        index = tfidf_summary_df.index,\n",
    "                                        columns = [\"similarity_score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is based on user ratings\n",
    "user_ratings_pivot = user_ratings.pivot(index = 'User',\n",
    "                                       columns = 'Book',\n",
    "                                       values = 'Rating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data by averaging the rating given by user, subtracted from the ratings for filling NA values\n",
    "# Get the average rating for each user \n",
    "avg_ratings = user_ratings_table.mean(axis=1)\n",
    "\n",
    "# Center each users ratings around 0\n",
    "user_ratings_table_centered = user_ratings_table.sub(avg_ratings, axis=0)\n",
    "\n",
    "# Fill in the missing data with 0s\n",
    "user_ratings_table_normed = user_ratings_table_centered.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Item based filtering\n",
    "+ Similar to the user-based filtering, we can also recommend based on item based\n",
    "+ If we transpose the data used for user-based, then we get Item based data\n",
    "+ User-based recommendations compare amongst users, and item-based recommendations compare different items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_ratings_pivot = user_ratings.pivot.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine similarities\n",
    "cosine_similarity(book_ratings_pivot.loc['Lord of the Rings',:].values.reshape(1, -1),\n",
    "                 book_ratings_pivot.loc['The Hobbit', :].values.reshape(1, -1))\n",
    "\n",
    "similarities = cosine_similarity(book_ratings_pivot)\n",
    "\n",
    "cosine_similarity_df = pd.Dataframe(similarities, \n",
    "                                   index = book_ratings_pivot.index,\n",
    "                                   columns = book_ratings_pivot.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### user-user similarity (Predicting the rating by a user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using K-Nearest Neighbors\n",
    "# Isolate the similarity scores for user_1 and sort\n",
    "user_similarity_series = user_similarities.loc['user_001']\n",
    "ordered_similarities = user_similarity_series.sort_values(ascending=False)\n",
    "\n",
    "# Find the top 10 most similar users\n",
    "nearest_neighbors = ordered_similarities[1:11].index\n",
    "\n",
    "# Extract the ratings of the neighbors\n",
    "neighbor_ratings = user_ratings_table.reindex(nearest_neighbors)\n",
    "\n",
    "# Calculate the mean rating given by the users nearest neighbors\n",
    "print(neighbor_ratings['Apollo 13 (1995)'].mean())\n",
    "\n",
    "# Drop the column you are trying to predict\n",
    "users_to_ratings.drop(\"Apollo 13 (1995)\", axis=1, inplace=True)\n",
    "\n",
    "# Get the data for the user you are predicting for\n",
    "target_user_x = users_to_ratings.loc[[\"user_001\"]]\n",
    "\n",
    "# Get the target data from user_ratings_table\n",
    "other_users_y = user_ratings_table['Apollo 13 (1995)']\n",
    "\n",
    "\n",
    "# target_user_x - Centered ratings that user_001 has given to the movies they have seen.\n",
    "# other_users_x - Centered ratings for all other users and the movies they have rated excluding the movie Apollo 13.\n",
    "# other_users_y - Raw ratings that all other users have given the movie Apollo 13.\n",
    "\n",
    "# Drop the column you are trying to predict\n",
    "users_to_ratings.drop(\"Apollo 13 (1995)\", axis=1, inplace=True)\n",
    "\n",
    "# Get the data for the user you are predicting for\n",
    "target_user_x = users_to_ratings.loc[[\"user_001\"]]\n",
    "\n",
    "# Get the target data from user_ratings_table\n",
    "other_users_y = user_ratings_table[\"Apollo 13 (1995)\"]\n",
    "\n",
    "# Get the data for only those that have seen the movie\n",
    "other_users_x = users_to_ratings[other_users_y.notnull()]\n",
    "\n",
    "# Remove those that have not seen the movie from the target\n",
    "other_users_y.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# Instantiate the user KNN model\n",
    "user_knn = KNeighborsRegressor(metric='cosine', n_neighbors=10)\n",
    "\n",
    "# Fit the model and predict the target user\n",
    "user_knn.fit(other_users_x, other_users_y)\n",
    "user_user_pred = user_knn.predict(target_user_x)\n",
    "\n",
    "print(user_user_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Item-based or User-based\n",
    "\n",
    "Item based\n",
    "+ Item-based recommendations are more consistent over time\n",
    "+ Easier to explain\n",
    "+ It can be pre-calculated\n",
    "\n",
    "Cons:\n",
    "+ It is very obivious recommendations\n",
    "\n",
    "User-based\n",
    "+ Can be a lot more interesting suggestions\n",
    "\n",
    "Cons:\n",
    "+ Generally beaten by item-based recommendations using standard metrics\n",
    "\n",
    "Item based for e-commerce stores (conservative)\n",
    "User based for books, movies (subjective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Comparing item-based and user-based models. You have now looked at two different KNN approaches. The first was item-item KNN where you use the average of the  most similar movies that a user has rated to suggest a rating for a movie they haven't watched. The other approach was user-user KNN where you use the average of the ratings that the  most similar users gave the movie to suggest what rating the target user would give the movie.\n",
    "\n",
    "Now, you will compare the two and calculate what rating user_002 would give to Forrest Gump.\n",
    "\n",
    "The code for the user_rating_predictor model (that predicts based on what similar users gave the movie), and the movie_rating_predictor (that predicts based off of what ratings this user gave to similar movies) has been started for you.\n",
    "\n",
    "KNeighborsRegressor has been imported for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the user KNN model\n",
    "user_knn = KNeighborsRegressor()\n",
    "\n",
    "# Fit the model and predict the target user\n",
    "user_knn.fit(other_users_x, other_users_y)\n",
    "user_user_pred = user_knn.predict(target_user_x)\n",
    "print(\"The user-user model predicts {}\".format(user_user_pred))\n",
    "\n",
    "# Instantiate the user KNN model\n",
    "movie_knn = KNeighborsRegressor()\n",
    "\n",
    "# Fit the model on the movie data and predict\n",
    "movie_knn.fit(other_movies_x, other_movies_y)\n",
    "item_item_pred = movie_knn.predict(target_movie_x)\n",
    "print(\"The item-item model predicts {}\".format(item_item_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sparsity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN will not perform well with sparse data\n",
    "# Count the occupied cells\n",
    "sparsity_count = user_ratings_df.isnull().values.sum()\n",
    "\n",
    "# Count all cells\n",
    "full_count = user_ratings_df.size\n",
    "\n",
    "# Find the sparsity of the DataFrame\n",
    "sparsity = sparsity_count / full_count\n",
    "print(sparsity)\n",
    "\n",
    "# Count the occupied cells per column\n",
    "occupied_count = user_ratings_df.notnull().sum()\n",
    "print(occupied_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occupied cells per column\n",
    "occupied_count = user_ratings_df.notnull().sum()\n",
    "\n",
    "# Sort the resulting series from low to high\n",
    "sorted_occupied_count = occupied_count.sort_values()\n",
    "\n",
    "# Plot a histogram of the values in sorted_occupied_count\n",
    "sorted_occupied_count.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix Factorization\n",
    "+ Factors can be found if there is atleast one value per row and column\n",
    "+ We can use this factors to get a filled dataframe\n",
    "+ The depth of the matrix will be equivalent to the number of users and width of the other factor will be equvivalent to items (in case of user-item collaborative filtering)\n",
    "+ We can decide the number of latent features (How to decide this?)\n",
    "+ There will be some amount of information loss because of this process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Multiply the user and item matrices\n",
    "predictions_df = np.dot(user_matrix, item_matrix)\n",
    "# Inspect the recreated DataFrame\n",
    "print(predictions_df)\n",
    "\n",
    "# Inspect the original DataFrame and compare\n",
    "print(original_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the average rating for each user \n",
    "avg_ratings = user_ratings_df.mean(axis=1)\n",
    "\n",
    "# Center each user's ratings around 0\n",
    "user_ratings_centered = user_ratings_df.sub(avg_ratings, axis=1)\n",
    "\n",
    "# Fill in all missing values with 0s\n",
    "user_ratings_centered.fillna(0, inplace=True)\n",
    "\n",
    "# Print the mean of each column\n",
    "print(user_ratings_centered.mean(axis=1))\n",
    "\n",
    "user_ratings_centered data you generated in the last exercise into 3 factors: U, sigma, and Vt.\n",
    "U is a matrix with a row for each user\n",
    "Vt has a column for each movie\n",
    "sigma is an array of weights that you will need to convert to a diagonal matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries \n",
    "from scipy.sparse.linalg import svds\n",
    "import numpy as np\n",
    "\n",
    "# Decompose the matrix\n",
    "U, sigma, Vt = svds(user_ratings_centered)\n",
    "\n",
    "# Convert sigma into a diagonal matrix\n",
    "sigma = np.diag(sigma)\n",
    "print(sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dot product of U and sigma\n",
    "U_sigma = np.dot(U, sigma)\n",
    "\n",
    "# Dot product of result and Vt\n",
    "U_sigma_Vt = np.dot(U_sigma, Vt)\n",
    "\n",
    "# Add back on the row means contained in avg_ratings\n",
    "uncentered_ratings = U_sigma_Vt + avg_ratings.values.reshape(-1, 1)\n",
    "\n",
    "# Create DataFrame of the results\n",
    "calc_pred_ratings_df = pd.DataFrame(uncentered_ratings, \n",
    "                                    index=user_ratings_df.index,\n",
    "                                    columns=user_ratings_df.columns\n",
    "                                   )\n",
    "# Print both the recalculated matrix and the original \n",
    "print(calc_pred_ratings_df)\n",
    "print(original_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the ground truth to compare your predictions against\n",
    "actual_values = act_ratings_df.iloc[:20, :100].values\n",
    "avg_values = avg_pred_ratings_df.iloc[:20, :100].values\n",
    "predicted_values = calc_pred_ratings_df.iloc[:20, :100].values\n",
    "\n",
    "# Create a mask of actual_values to only look at the non-missing values in the ground truth\n",
    "mask = ~np.isnan(actual_values)\n",
    "\n",
    "# Print the performance of both predictions and compare\n",
    "print(mean_squared_error(actual_values[mask], avg_values[mask], squared=False))\n",
    "print(mean_squared_error(actual_values[mask], predicted_values[mask], squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
