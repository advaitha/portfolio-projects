{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etl_dag = DAG (dag_id = 'etl_pipeline',\n",
    "              default_args = {'start_date':'2020-01-08'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# airflow run command (shell)\n",
    "airflow run <dag_id> <task_id> <start_date> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import DAG\n",
    "from datetime import datetime\n",
    "\n",
    "default_arguments = {\n",
    "    'owner':'Thulasiram',\n",
    "    'email':'tulasiram.gunipati@gmail.com',\n",
    "    'start_date':datetime(2020, 1, 20),\n",
    "    'retries':2\n",
    "}\n",
    "\n",
    "etl_dag = DAG('etl_workflow', default_args = default_arguments)\n",
    "\n",
    "# airflow -h for descriptions\n",
    "# airflow list_dags to show all recognized DAGs\n",
    "\n",
    "part1 = BashOperator(\n",
    "    task_id = 'generate_random_number'\n",
    "    bash_command = 'echo $RANDOM',\n",
    "    dag = dag\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Airflow command to start the server\n",
    "airflow webserver -p port_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import DAG\n",
    "from airflow.operators.bash_opearator import BashOperator\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from airflow.operators.http_operator import SimpleHttpOperator\n",
    "\n",
    "dag = DAG(\n",
    "dag_id = 'update_start',\n",
    "default_args = {\"start_date\":\"2019-10-01\"})\n",
    "\n",
    "part1 = BashOperator(\n",
    "    task_id = 'generate_random_number',\n",
    "    bash_command = 'echo $RANDOM',\n",
    "    dag = dag   \n",
    ")\n",
    "\n",
    "import sys\n",
    "def python_version():\n",
    "    return sys.version\n",
    "\n",
    "part2 = PythonOperator(\n",
    "    task_id = 'get_python_version',\n",
    "    python_callable = python_version,\n",
    "    dag = dag\n",
    ")\n",
    "\n",
    "part3 = SimpleHttpOperator(\n",
    "    task_id = 'query_server_for_external_ip',\n",
    "    endpoint = 'https://api.ipify.org',\n",
    "    method = 'GET',\n",
    "    dag = dag\n",
    ")\n",
    "\n",
    "part 3 >> part2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "+ Operators are available in the airflow.operators / airflow.contrib.operators libraries\n",
    "+ Represents a single task in a workflow\n",
    "+ Run independently (usually)\n",
    "+ Generally do not share information\n",
    "+ Various operators to perform different tasks\n",
    "+ using BatchOperator we can move from running individual bash scripts to airflow "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Instances of operators\n",
    "+ Task Dependencies\n",
    "+ Reffered to upstream or downstream\n",
    "+ Defined using bitshift operators\n",
    "+ >> upstream operator\n",
    "+ << downstream operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new pull_sales task\n",
    "pull_sales = BashOperator(\n",
    "    task_id='pullsales_task',\n",
    "    bash_command = 'wget https://salestracking/latestinfo?json',\n",
    "    dag=analytics_dag\n",
    ")\n",
    "\n",
    "# Set pull_sales to run prior to cleanup\n",
    "pull_sales >> cleanup\n",
    "\n",
    "# Configure consolidate to run after cleanup\n",
    "cleanup >> consolidate\n",
    "\n",
    "# Set push_data to run last\n",
    "consolidate >> push_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.operators.python_operator import PythonOperator\n",
    "def sleep(length_of_time):\n",
    "    time.sleep(length_of_time)\n",
    "    \n",
    "sleep_task = PythonOperator(\n",
    "    task_id = 'sleep',\n",
    "    python_callable = sleep,\n",
    "    op_kwargs = {'length_of_time':5},\n",
    "    dag = example_dag\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Airflow contain emailOperator as well\n",
    "from airflow.operators.email_operator import EmailOperator\n",
    "\n",
    "email_task = EmailOperator(\n",
    "    task_id = 'email_sales_report',\n",
    "    to = 'sales_manager@example.com',\n",
    "    subject = \"Automated report\",\n",
    "    html_content = 'Attached is the latest report',\n",
    "    files = 'latest_sales.xlsx',\n",
    "    dag = example_dag\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to download data from url\n",
    "\n",
    "def pull_file(URL, savepath):\n",
    "    r = requests.get(URL)\n",
    "    with open(savepath, 'wb') as f:\n",
    "        f.write(r.content)   \n",
    "    # Use the print method for logging\n",
    "    print(f\"File pulled from {URL} and saved to {savepath}\")\n",
    "\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "\n",
    "# Create the task\n",
    "pull_file_task = PythonOperator(\n",
    "    task_id='pull_file',\n",
    "    # Add the callable\n",
    "    python_callable=pull_file,\n",
    "    # Define the arguments\n",
    "    op_kwargs={'URL':'http://dataserver/sales.json', 'savepath':'latestsales.json'},\n",
    "    dag=process_sales_dag\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scheduling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date - Date / time to initially schedule the DAG run\n",
    "end_date - Optional attribute for when to stop running new DAG instances\n",
    "max_tries - How many attempts to make\n",
    "schedule_interval - How often to run - cron syntax or built in presets\n",
    "\n",
    "#### cron syntax\n",
    "minute - (0 - 59)\n",
    "hour - (0 - 23)\n",
    "day of the month - (1 - 31)\n",
    "month - (1 - 12)\n",
    "day of the week - (0 - 6)(sunday to saturday)\n",
    "\n",
    "# preset    cron equivalent\n",
    "@hourly     0 * * * *\n",
    "@daily      0 0 * * *\n",
    "@weekly     0 0 * * 0\n",
    "None - used for manual triggered DAGs\n",
    "@once - schedule only once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the scheduling arguments as defined\n",
    "default_args = {\n",
    "  'owner': 'Engineering',\n",
    "  'start_date': datetime(2019, 11, 1),\n",
    "  'email': ['airflowresults@datacamp.com'],\n",
    "  'email_on_failure': False,\n",
    "  'email_on_retry': False,\n",
    "  'retries': 3,\n",
    "  'retry_delay': timedelta(minutes=20)\n",
    "}\n",
    "\n",
    "# schedule for every wednesday at 12.30\n",
    "dag = DAG('update_dataflows', default_args=default_args, schedule_interval='30 12 * * 3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ An operator that waits for a certain condition to be true\n",
    "+ Can be defined how often to check for the condition to be true\n",
    "+ They are a type of operators and can be assigned to tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.sensors.base_sensor_operator\n",
    "mode = 'poke' run repeatedly\n",
    "mode = 'reschedule' - Give up a task slot and try again later\n",
    "timeout in seconds\n",
    "Sensors also consists of operator agruments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File sensor\n",
    "from airflow.contrib.sensors.file_sensor import FileSensor\n",
    "file_sensor_task = FileSensor(task_id = 'file_sense',\n",
    "                             filepath = 'salesdata.csv',\n",
    "                             poke_interval = 300,\n",
    "                             dag = sales_report_dag)\n",
    "init_sales_cleanup >> file_sensor_task >> generate_report\n",
    "\n",
    "# ExternalTaskSensor - wait for a task in another DAG to complete\n",
    "# HttpSensor - Request a web URL and check for content\n",
    "# SqlSensor - Runs a SQL query to check for content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### When to use a Sensor\n",
    "+ Uncertain when it will be true\n",
    "+ If failure not immediately desired\n",
    "+ To add task repetition without loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Executors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "+ Executors run tasks\n",
    "+ Example executors - SequentialExecutor, LocalExecutor, CeleryExecutor\n",
    "+ We can know the type of executor by looking at the airflow.cfg file\n",
    "+ cat airflow/airflow.cfg | grep \"executor =\"\n",
    "+ we can also know this from airflow list_dags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SLA's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "+ Use 'sla' argument on the task\n",
    "task 1 = BashOperator(task_id = 'sla_task',\n",
    "                     bash_command = 'runcode.sh',\n",
    "                     sla = timedelta(seconds = 30),\n",
    "                     dag = dag)\n",
    "default_args = {\n",
    "    'sla': timedelta(minutes = 20),\n",
    "    'start_date':datetime(2020,2,20)\n",
    "}\n",
    "\n",
    "dag = DAG('sla_dag', default_args = default_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the timedelta object\n",
    "from datetime import timedelta\n",
    "\n",
    "# Create the dictionary entry\n",
    "default_args = {\n",
    "  'start_date': datetime(2020, 2, 20),\n",
    "  'sla': timedelta(minutes = 30)\n",
    "}\n",
    "\n",
    "# Add to the DAG\n",
    "test_dag = DAG('test_workflow', default_args=default_args, schedule_interval='@None')\n",
    "\n",
    "\n",
    "# Import the timedelta object\n",
    "from datetime import timedelta\n",
    "\n",
    "test_dag = DAG('test_workflow', start_date=datetime(2020,2,20), schedule_interval='@None')\n",
    "\n",
    "# Create the task with the SLA\n",
    "task1 = BashOperator(task_id='first_task',\n",
    "                     sla=timedelta(hours = 3),\n",
    "                     bash_command='initialize_data.sh',\n",
    "                     dag=test_dag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the email task\n",
    "email_report = EmailOperator(\n",
    "        task_id='email_report',\n",
    "        to='airflow@datacamp.com',\n",
    "        subject='Airflow Monthly Report',\n",
    "        html_content=\"\"\"Attached is your monthly workflow report - please refer to it for more detail\"\"\",\n",
    "        files=['monthly_report.pdf'],\n",
    "        dag=report_dag\n",
    ")\n",
    "\n",
    "# Set the email task to run after the report is generated\n",
    "email_report << generate_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import DAG\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "from airflow.contrib.sensors.file_sensor import FileSensor\n",
    "from datetime import datetime\n",
    "\n",
    "default_args={\n",
    "    'email_on': ['airflowalerts@datacamp.com','airflowadmin@datacamp.com'],\n",
    "    'on_failure': True,\n",
    "    'on_success': True\n",
    "}\n",
    "report_dag = DAG(\n",
    "    dag_id = 'execute_report',\n",
    "    schedule_interval = \"0 0 * * *\",\n",
    "    default_args=default_args\n",
    ")\n",
    "\n",
    "precheck = FileSensor(\n",
    "    task_id='check_for_datafile',\n",
    "    filepath='salesdata_ready.csv',\n",
    "    start_date=datetime(2020,2,20),\n",
    "    mode='reschedule',\n",
    "    dag=report_dag)\n",
    "\n",
    "generate_report_task = BashOperator(\n",
    "    task_id='generate_report',\n",
    "    bash_command='generate_report.sh',\n",
    "    start_date=datetime(2020,2,20),\n",
    "    dag=report_dag\n",
    ")\n",
    "\n",
    "precheck >> generate_report_task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Allow substituting information during a DAG run\n",
    "+ Provide added flexibility when defining tasks\n",
    "+ Are created using the jinja templating language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "templated_command=\"\"\"\n",
    "    echo \"Reading {{params.filename}}\"\n",
    "\"\"\"\n",
    "\n",
    "t1 = BashOperator(task_id = 'template_task',\n",
    "                 bash_command = templated_command,\n",
    "                 params = {'filename':'file1.txt'},\n",
    "                 dag = example_dag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import DAG\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "from datetime import datetime\n",
    "\n",
    "default_args = {\n",
    "  'start_date': datetime(2020, 4, 15),\n",
    "}\n",
    "\n",
    "cleandata_dag = DAG('cleandata',\n",
    "                    default_args=default_args,\n",
    "                    schedule_interval='@daily')\n",
    "\n",
    "# Create a templated command to execute\n",
    "# 'bash cleandata.sh datestring'\n",
    "\n",
    "templated_command=\"\"\"\n",
    "    bash cleandata.sh {{ds_nodash}}\n",
    "\"\"\"\n",
    "\n",
    "# Modify clean_task to use the templated command\n",
    "clean_task = BashOperator(task_id='cleandata_task',\n",
    "                          bash_command=templated_command,\n",
    "                          dag=cleandata_dag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import DAG\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "from datetime import datetime\n",
    "\n",
    "default_args = {\n",
    "  'start_date': datetime(2020, 4, 15),\n",
    "}\n",
    "\n",
    "cleandata_dag = DAG('cleandata',\n",
    "                    default_args=default_args,\n",
    "                    schedule_interval='@daily')\n",
    "\n",
    "# Modify the templated command to handle a\n",
    "# second argument called filename.\n",
    "templated_command = \"\"\"\n",
    "  bash cleandata.sh {{ ds_nodash }} {{params.filename}}\n",
    "\"\"\"\n",
    "\n",
    "# Modify clean_task to pass the new argument\n",
    "clean_task = BashOperator(task_id='cleandata_task',\n",
    "                          bash_command=templated_command,\n",
    "                          params={'filename': 'salesdata.txt'},\n",
    "                          dag=cleandata_dag)\n",
    "\n",
    "# Create a new BashOperator clean_task2\n",
    "clean_task2 = BashOperator(task_id='cleandata_task2',\n",
    "                           bash_command = templated_command,\n",
    "                           params = {'filename':'supportdata.txt'},\n",
    "                           dag = cleandata_dag)\n",
    "                           \n",
    "# Set the operator dependencies\n",
    "clean_task >> clean_task2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jinja templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "templated_command = \"\"\"\n",
    "{% for filename in params.filenames %}\n",
    "    echo \"Reading {{ filename }}\"\n",
    "{% endfor %}\n",
    "\"\"\"\n",
    "\n",
    "t1 = BashOperator(task_id = 'template_task',\n",
    "                 bash_command = templated_command,\n",
    "                 params = {'filenames': ['file1.txt','file2.txt']}\n",
    "                 dag = example_dag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import DAG\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "from datetime import datetime\n",
    "\n",
    "filelist = [f'file{x}.txt' for x in range(30)]\n",
    "\n",
    "default_args = {\n",
    "  'start_date': datetime(2020, 4, 15),\n",
    "}\n",
    "\n",
    "cleandata_dag = DAG('cleandata',\n",
    "                    default_args=default_args,\n",
    "                    schedule_interval='@daily')\n",
    "\n",
    "# Modify the template to handle multiple files in a \n",
    "# single run.\n",
    "templated_command = \"\"\"\n",
    "  <% for filename in params.filenames %>\n",
    "  bash cleandata.sh {{ ds_nodash }} {{ filename }};\n",
    "  <% endfor %>\n",
    "\"\"\"\n",
    "\n",
    "# Modify clean_task to use the templated command\n",
    "clean_task = BashOperator(task_id='cleandata_task',\n",
    "                          bash_command=templated_command,\n",
    "                          params={'filenames': filelist},\n",
    "                          dag=cleandata_dag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import DAG\n",
    "from airflow.operators.email_operator import EmailOperator\n",
    "from datetime import datetime\n",
    "\n",
    "# Create the string representing the html email content\n",
    "html_email_str = \"\"\"\n",
    "Date: {{ ds }}\n",
    "Username: {{ params.username }}\n",
    "\"\"\"\n",
    "\n",
    "email_dag = DAG('template_email_test',\n",
    "                default_args={'start_date': datetime(2020, 4, 15)},\n",
    "                schedule_interval='@weekly')\n",
    "                \n",
    "email_task = EmailOperator(task_id='email_task',\n",
    "                           to='testuser@datacamp.com',\n",
    "                           subject=\"{{ macros.uuid.uuid4() }}\",\n",
    "                           html_content=html_email_str,\n",
    "                           params={'username': 'testemailuser'},\n",
    "                           dag=email_dag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Branching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def branch_test(**kwargs):\n",
    "    if int(kwargs['ds_nodash']) % 2 == 0:\n",
    "        return 'even_day_task'\n",
    "    else:\n",
    "        return 'odd_day_task'\n",
    "    \n",
    "branch_task = BranchPythonOperator(task_id = 'branch_task', dag = dag,\n",
    "                                  provide_context = True,\n",
    "                                  python_callable=branch_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to determine if years are different\n",
    "def year_check(**kwargs):\n",
    "    current_year = int(kwargs['ds_nodash'][0:4])\n",
    "    previous_year = int(kwargs['prev_ds_nodash'][0:4])\n",
    "    if current_year == previous_year:\n",
    "        return 'current_year_task'\n",
    "    else:\n",
    "        return 'new_year_task'\n",
    "\n",
    "# Define the BranchPythonOperator\n",
    "branch_task = BranchPythonOperator(task_id='branch_task', dag=branch_dag,\n",
    "                                   python_callable=year_check, provide_context=True)\n",
    "# Define the dependencies\n",
    "branch_dag >> current_year_task\n",
    "branch_dag >> new_year_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import DAG\n",
    "from airflow.operators.python_operator import BranchPythonOperator\n",
    "from airflow.operators.dummy_operator import DummyOperator\n",
    "from datetime import datetime\n",
    "\n",
    "dag = DAG('BranchingTest', default_args={'start_date': datetime(2020, 4, 15)}, schedule_interval='@daily')\n",
    "\n",
    "def branch_test(**kwargs):\n",
    "  if int(kwargs['ds_nodash']) % 2 == 0:\n",
    "    return 'even_day_task'\n",
    "  else:\n",
    "    return 'odd_day_task'\n",
    " \n",
    "start_task = DummyOperator(task_id='start_task', dag=dag)\n",
    "\n",
    "branch_task = BranchPythonOperator(\n",
    "       task_id='branch_task',\n",
    "       provide_context=True,\n",
    "       python_callable=branch_test,\n",
    "       dag=dag)\n",
    "\n",
    "even_day_task = DummyOperator(task_id='even_day_task', dag=dag)\n",
    "even_day_task2 = DummyOperator(task_id='even_day_task2', dag=dag)\n",
    "\n",
    "odd_day_task = DummyOperator(task_id='odd_day_task', dag=dag)\n",
    "odd_day_task2 = DummyOperator(task_id='odd_day_task2', dag=dag)\n",
    "\n",
    "start_task >> branch_task \n",
    "even_day_task >> even_day_task2\n",
    "odd_day_task >> odd_day_task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run a specific task from command-line\n",
    "airflow run <dag_id> <task_id> <date>\n",
    "\n",
    "# To run a full DAG:\n",
    "airflow trigger_dag -e <date> <dag_id>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import DAG\n",
    "from airflow.contrib.sensors.file_sensor import FileSensor\n",
    "\n",
    "# Import the needed operators\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from datetime import date, datetime\n",
    "\n",
    "def process_data(**context):\n",
    "  file = open('/home/repl/workspace/processed_data.tmp', 'w')\n",
    "  file.write(f'Data processed on {date.today()}')\n",
    "  file.close()\n",
    "\n",
    "    \n",
    "dag = DAG(dag_id='etl_update', default_args={'start_date': datetime(2020,4,1)})\n",
    "\n",
    "sensor = FileSensor(task_id='sense_file', \n",
    "                    filepath='/home/repl/workspace/startprocess.txt',\n",
    "                    poke_interval=5,\n",
    "                    timeout=15,\n",
    "                    dag=dag)\n",
    "\n",
    "bash_task = BashOperator(task_id='cleanup_tempfiles', \n",
    "                         bash_command='rm -f /home/repl/*.tmp',\n",
    "                         dag=dag)\n",
    "\n",
    "python_task = PythonOperator(task_id='run_processing', \n",
    "                             python_callable=process_data,\n",
    "                             dag=dag)\n",
    "\n",
    "sensor >> bash_task >> python_task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import DAG\n",
    "from airflow.contrib.sensors.file_sensor import FileSensor\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from dags.process import process_data\n",
    "from datetime import timedelta, datetime\n",
    "\n",
    "# Update the default arguments and apply them to the DAG\n",
    "default_args = {\n",
    "  'start_date': datetime(2019,1,1),\n",
    "  'sla':timedelta(minutes = 90)\n",
    "}\n",
    "\n",
    "dag = DAG(dag_id='etl_update', default_args=default_args)\n",
    "\n",
    "sensor = FileSensor(task_id='sense_file', \n",
    "                    filepath='/home/repl/workspace/startprocess.txt',\n",
    "                    poke_interval = 45,\n",
    "                    dag=dag)\n",
    "\n",
    "bash_task = BashOperator(task_id='cleanup_tempfiles', \n",
    "                         bash_command='rm -f /home/repl/*.tmp',\n",
    "                         dag=dag)\n",
    "\n",
    "python_task = PythonOperator(task_id='run_processing', \n",
    "                             python_callable=process_data,\n",
    "                             provide_context = True,\n",
    "                             dag=dag)\n",
    "\n",
    "sensor >> bash_task >> python_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import DAG\n",
    "from airflow.contrib.sensors.file_sensor import FileSensor\n",
    "from airflow.operators.bash_operator import BashOperator\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from airflow.operators.python_operator import BranchPythonOperator\n",
    "from airflow.operators.dummy_operator import DummyOperator\n",
    "from airflow.operators.email_operator import EmailOperator\n",
    "from dags.process import process_data\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Update the default arguments and apply them to the DAG.\n",
    "\n",
    "default_args = {\n",
    "  'start_date': datetime(2019,1,1),\n",
    "  'sla': timedelta(minutes=90)\n",
    "}\n",
    "    \n",
    "dag = DAG(dag_id='etl_update', default_args=default_args)\n",
    "\n",
    "sensor = FileSensor(task_id='sense_file', \n",
    "                    filepath='/home/repl/workspace/startprocess.txt',\n",
    "                    poke_interval=45,\n",
    "                    dag=dag)\n",
    "\n",
    "bash_task = BashOperator(task_id='cleanup_tempfiles', \n",
    "                         bash_command='rm -f /home/repl/*.tmp',\n",
    "                         dag=dag)\n",
    "\n",
    "python_task = PythonOperator(task_id='run_processing', \n",
    "                             python_callable=process_data,\n",
    "                             provide_context=True,\n",
    "                             dag=dag)\n",
    "\n",
    "\n",
    "email_subject=\"\"\"\n",
    "  Email report for {{ params.department }} on {{ ds_nodash }}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "email_report_task = EmailOperator(task_id='email_report_task',\n",
    "                                  to='sales@mycompany.com',\n",
    "                                  subject=email_subject,\n",
    "                                  html_content='email_subject',\n",
    "                                  params={'department': 'Data subscription services'},\n",
    "                                  dag=dag)\n",
    "\n",
    "\n",
    "no_email_task = DummyOperator(task_id='no_email_task', dag=dag)\n",
    "\n",
    "\n",
    "def check_weekend(**kwargs):\n",
    "    dt = datetime.strptime(kwargs['execution_date'],\"%Y-%m-%d\")\n",
    "    # If dt.weekday() is 0-4, it's Monday - Friday. If 5 or 6, it's Sat / Sun.\n",
    "    if (dt.weekday() < 5):\n",
    "        return 'email_report_task'\n",
    "    else:\n",
    "        return 'no_email_task'\n",
    "    \n",
    "    \n",
    "branch_task = BranchPythonOperator(task_id='check_if_weekend',\n",
    "                                   python_callable = check_weekend,\n",
    "                                   provide_context = True,\n",
    "                                   dag=dag)\n",
    "\n",
    "    \n",
    "sensor >> bash_task >> python_task\n",
    "\n",
    "python_task >> branch_task >> [email_report_task, no_email_task]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
