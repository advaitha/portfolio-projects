{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning in PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Schemas\n",
    "+ May contain various data types\n",
    "+ Can filter garbage data during import\n",
    "+ Improves read performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.types\n",
    "peopleSchema = StructType([\n",
    "    StructField('name', StringType(), True), # Boolean is if data can be null or not\n",
    "    StructField('age', IntegerType(), True),\n",
    "    StructField('city',StringType(), True)    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_df = spark.read.format('csv').load(name = 'rawdata.csv', \n",
    "                                          schema = peopleSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pyspark.sql.types library\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Define a new schema using the StructType method\n",
    "people_schema = StructType([\n",
    "  # Define a StructField for each field\n",
    "  StructField('name',StringType(), False),\n",
    "  StructField('age', IntegerType(),False),\n",
    "  StructField('city',StringType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "aa_dfw_df = spark.read.format('csv').options(Header=True).load('AA_DFW_2018.csv.gz')\n",
    "\n",
    "# Add the airport column using the F.lower() method\n",
    "aa_dfw_df = aa_dfw_df.withColumn('airport', F.lower(aa_dfw_df['Destination Airport']))\n",
    "\n",
    "# Drop the Destination Airport column\n",
    "aa_dfw_df = aa_dfw_df.drop(aa_dfw_df['Destination Airport'])\n",
    "\n",
    "# Show the DataFrame\n",
    "aa_dfw_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Difficulties with CSV files\n",
    "+ No Defined schema\n",
    "+ Nested data requires special handling\n",
    "+ Slow to parse\n",
    "+ If schema is not provided, all data must be read before inferring schema\n",
    "+ Predicate pushdown - Ordering tasks to do least amount of work. Filtering the data before processing is one of the optimizations for large datasets. In CSV this cannot be done.\n",
    "+ Any intermediate use requires redefining schemas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parquet Format\n",
    "+ A Columnar data format\n",
    "+ Supported in Spark \n",
    "+ Supports predicate pushdown\n",
    "+ Automatically stores schema information\n",
    "+ Binary file format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading parquet files\n",
    "df = spark.read.format('parquet').load('filename.parquet')\n",
    "df = spark.read.parquet('filename.parquet')\n",
    "df.createOrReplaceTempView('flights')\n",
    "\n",
    "# Writing parquet files\n",
    "df.write.format('parquet').save('filename.parquet')\n",
    "df.write.parquet('filename.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the row count of df1 and df2\n",
    "print(\"df1 Count: %d\" % df1.count())\n",
    "print(\"df2 Count: %d\" % df2.count())\n",
    "\n",
    "# Combine the DataFrames into one\n",
    "df3 = df1.union(df2)\n",
    "\n",
    "# Save the df3 DataFrame in Parquet format\n",
    "df3.write.parquet('AA_DFW_ALL.parquet', mode='overwrite')\n",
    "\n",
    "# Read the Parquet file into a new DataFrame and run a count\n",
    "print(spark.read.parquet('AA_DFW_ALL.parquet').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Parquet file into flights_df\n",
    "flights_df = spark.read.parquet('AA_DFW_ALL.parquet')\n",
    "\n",
    "# Register the temp table\n",
    "flights_df.createOrReplaceTempView('flights')\n",
    "\n",
    "# Run a SQL query of the average flight duration\n",
    "avg_duration = spark.sql('SELECT avg(flight_duration) from flights').collect()[0]\n",
    "print('The average flight time is: %d' % avg_duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulating DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter / Where\n",
    "voter_df.filter(voter_df.date > '1/1/2019')\n",
    "voter_df.where(voter_df.date > '1/1/2019')\n",
    "voter_df.filter(voter_df['name'].isNotNUll())  # Remove Nulls\n",
    "voter_df.where(~ voter_df['name'].isNull())\n",
    "voter_df.filter(voter_df.date.year > 1800) \n",
    "voter_df.where(voter_df['_C0'].contains('VOTE')) \n",
    "voter_df.where(~voter_df._c1.isNull()) # using Negation\n",
    "\n",
    "# Select\n",
    "voter_df.select(voter_df.name)\n",
    "\n",
    "# withColumn to create new column - (name_of_column, command to create)\n",
    "voter_df.withColumn('year', voter_df.date.year)\n",
    "\n",
    "# drop\n",
    "voter_df.drop('unused_column')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# String transformations contained in pyspark.sql.functions\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Applied per column\n",
    "voter_df.withColumn('lower', F.lower('name'))\n",
    "voter_df.withColumn('splits', F.split('name',' ')) # intermediary columns\n",
    "voter_df.withColumn('year', voter_df['_c4'].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ArrayType\n",
    ".size(column) # length of array type column\n",
    ".getItem(index) # Retrieves the items with the index from the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the distinct VOTER_NAME entries\n",
    "voter_df.select('VOTER_NAME').distinct().show(40, truncate=False)\n",
    "\n",
    "# Filter voter_df where the VOTER_NAME is 1-20 characters in length\n",
    "voter_df = voter_df.filter('length(VOTER_NAME) > 0 and length(VOTER_NAME) < 20')\n",
    "\n",
    "# Filter out voter_df where the VOTER_NAME contains an underscore\n",
    "voter_df = voter_df.filter(~ F.col('VOTER_NAME').contains('_'))\n",
    "\n",
    "# Show the distinct VOTER_NAME entries again\n",
    "voter_df.select('VOTER_NAME').distinct().show(40, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column called splits separated on whitespace\n",
    "voter_df = voter_df.withColumn('splits', F.split(voter_df.VOTER_NAME, '\\s+'))\n",
    "\n",
    "# Create a new column called first_name based on the first item in splits\n",
    "voter_df = voter_df.withColumn('first_name', voter_df.splits.getItem(0))\n",
    "\n",
    "# Get the last entry of the splits list and create a column called last_name\n",
    "voter_df = voter_df.withColumn('last_name', voter_df.splits.getItem(F.size('splits') - (F.size('splits')-1)))\n",
    "\n",
    "# Drop the splits column\n",
    "voter_df = voter_df.drop('splits')\n",
    "\n",
    "# Show the voter_df DataFrame\n",
    "voter_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conditionals\n",
    "# .when(if_condition, then)\n",
    "df.select(df.Name, df.Age, F.when(df.Age >= 18, \"Adult\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(df.Name, df.Age,\n",
    "         F.when(df.Age >= 18, \"Adult\")\n",
    "          .when(df.Age < 18, \"Minor\"))\n",
    "\n",
    "# otherwise() is like else\n",
    "df.select(df.Name, df.Age,\n",
    "         F.when(df.Age >= 18, \"Adult\")\n",
    "          .otherwise(\"Minor\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column to voter_df for any voter with the title **Councilmember**\n",
    "voter_df = voter_df.withColumn('random_val',\n",
    "                               when(voter_df.TITLE == 'Councilmember', F.rand()))\n",
    "\n",
    "# Show some of the DataFrame rows, noting whether the when clause worked\n",
    "voter_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column to voter_df for a voter based on their position\n",
    "voter_df = voter_df.withColumn('random_val',\n",
    "                               F.when(voter_df.TITLE == 'Councilmember', F.rand())\n",
    "                               .when(voter_df.TITLE == 'Mayor', 2)\n",
    "                               .otherwise(0))\n",
    "\n",
    "# Show some of the DataFrame rows\n",
    "voter_df.show()\n",
    "\n",
    "# Use the .filter() clause with random_val\n",
    "voter_df.filter(voter_df.random_val == 0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "+ Python Method\n",
    "+ wrapped via pyspark.sql.functions.udf method\n",
    "+ stored as a variable and called like a normal spark function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a python method\n",
    "def reverseString(mystr):\n",
    "    return mystr[::-1]\n",
    "\n",
    "#wrap the function and store as a variable\n",
    "udfReverseString = F.udf(reverseString, StringType())\n",
    "\n",
    "#Use with Spark\n",
    "user_df = user_df_withColumn('ReverseName', udfReverseString(user_df.Name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getFirstAndMiddle(names):\n",
    "  # Return a space separated string of names\n",
    "  return ' '.join(names[:-1])\n",
    "\n",
    "# Define the method as a UDF\n",
    "udfFirstAndMiddle = F.udf(getFirstAndMiddle, StringType())\n",
    "\n",
    "# Create a new column using your UDF\n",
    "voter_df = voter_df.withColumn('first_and_middle_name', udfFirstAndMiddle(voter_df.splits))\n",
    "\n",
    "# Show the DataFrame\n",
    "voter_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assigning IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Sequential ID's create bottleneck in spark\n",
    "+ Monotonically increasing IDs can be used in spark\n",
    "+ pyspark.sql.functions.monotonically_increasing_id()\n",
    "+ This is not sequential and completely parallel\n",
    "+ IDs are provided based on the partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all the unique council voters\n",
    "voter_df = df.select(df[\"VOTER NAME\"]).distinct()\n",
    "\n",
    "# Count the rows in voter_df\n",
    "print(\"\\nThere are %d rows in the voter_df DataFrame.\\n\" % voter_df.count())\n",
    "\n",
    "# Add a ROW_ID\n",
    "voter_df = voter_df.withColumn('ROW_ID', F.monotonically_increasing_id())\n",
    "\n",
    "# Show the rows with 10 highest IDs in the set\n",
    "voter_df.orderBy(voter_df.ROW_ID.desc()).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
