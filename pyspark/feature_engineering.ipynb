{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return spark version\n",
    "print(spark.version)\n",
    "\n",
    "# Return python version\n",
    "import sys\n",
    "print(sys.version_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_load(df, num_records, num_columns):\n",
    "  # Takes a dataframe and compares record and column counts to input\n",
    "  # Message to return if the critera below aren't met\n",
    "  message = 'Validation Failed'\n",
    "  # Check number of records\n",
    "  if num_records == df.count():\n",
    "    # Check number of columns\n",
    "    if num_columns == len(df.columns):\n",
    "      # Success message\n",
    "      message = 'validation passed'\n",
    "  return message\n",
    "\n",
    "# Print the data validation message\n",
    "print(check_load(df, 5000, 74))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of actual dtypes to check\n",
    "actual_dtypes_list = df.dtypes\n",
    "print(actual_dtypes_list)\n",
    "\n",
    "# Iterate through the list of actual dtypes tuples\n",
    "for attribute_tuple in actual_dtypes_list:\n",
    "  \n",
    "  # Check if column name is dictionary of expected dtypes\n",
    "  col_name = attribute_tuple[0]\n",
    "  if col_name in validation_dict:\n",
    "\n",
    "    # Compare attribute types\n",
    "    col_type = attribute_tuple[1]\n",
    "    if col_type == validation_dict[col_name]:\n",
    "      print(col_name + ' has expected dtype.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To calculate mean\n",
    "df.agg({'SALESCLOSEPRICE':'mean'}).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To calculate covariance\n",
    "df.cov('SALESCLOSEPRICE','YEARBUILT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling\n",
    "df.sample(False,0.5,42) #Arguments - (withreplacement, fraction, seed)\n",
    "df_pd = df.toPandas()\n",
    "sns.distplot(variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking correlation\n",
    "# Name and value of col with max corr\n",
    "corr_max = 0\n",
    "corr_max_col = columns[0]\n",
    "\n",
    "# Loop to check all columns contained in list\n",
    "for col in columns:\n",
    "    # Check the correlation of a pair of columns\n",
    "    corr_val = df.corr(col, 'SALESCLOSEPRICE')\n",
    "    # Logic to compare corr_max with current corr_val\n",
    "    if corr_val > corr_max:\n",
    "        # Update the column name and corr value\n",
    "        corr_max = corr_val\n",
    "        corr_max_col = col\n",
    "\n",
    "print(corr_max_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a single column and sample and convert to pandas\n",
    "sample_df = df.select(['LISTPRICE']).sample(False, 0.5, 42)\n",
    "pandas_df = sample_df.toPandas()\n",
    "\n",
    "# Plot distribution of pandas_df and display plot\n",
    "sns.distplot(pandas_df)\n",
    "plt.show()\n",
    "\n",
    "# Import skewness function\n",
    "from pyspark.sql.functions import skewness\n",
    "\n",
    "# Compute and print skewness of LISTPRICE\n",
    "print(df.agg({'LISTPRICE': 'skewness'}).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a the relevant columns and sample\n",
    "sample_df = df.select(['SALESCLOSEPRICE', 'LIVINGAREA']).sample(False, 0.5, 42)\n",
    "\n",
    "# Convert to pandas dataframe\n",
    "pandas_df = sample_df.toPandas()\n",
    "\n",
    "# Linear model plot of pandas_df\n",
    "sns.lmplot(x='LIVINGAREA', y='SALESCLOSEPRICE', data=pandas_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of columns to drop\n",
    "cols_to_drop = ['NO', 'UNITNUMBER', 'CLASS']\n",
    "df = df.drop(*cols_to_drop) # '*' is required\n",
    "\n",
    "# like - a sql LIKE pattern returns boolean column\n",
    "df = df.where(~df['POTENTIALSHORTSALE'].like('Not Disclosed'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outliers - more than three standard deviations from the mean. Take care of both the sides. Data filtered should not be more than 3%\n",
    "std_val = df.agg({'LISTPRICE':'stddev'}).collect()[0][0]\n",
    "mean_val = df.agg({'LISTPRICE':'mean'}).collect()[0][0]\n",
    "hi_bound = mean_val + (3 * std_val)\n",
    "low_bound = mean_val - (3 * std_val)\n",
    "df = df.where((df['LISTPRICE'] < hi_bound) & (df['LISTPRICE'] > low_bound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping NA\n",
    "df = df.dropna()\n",
    "df = df.dropna(how = 'all', subset['LISTPRICE','SALECLOSEPRICE'])\n",
    "df = df.dropna(thresh = 2) # where atleast 2 columns have NULL values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping Duplicates\n",
    "# After dropping columns or joining datasets\n",
    "df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect unique values in the column 'ASSUMABLEMORTGAGE'\n",
    "df.select(['ASSUMABLEMORTGAGE']).distinct().show()\n",
    "\n",
    "# List of possible values containing 'yes'\n",
    "yes_values = ['Yes w/ Qualifying', 'Yes w/No Qualifying']\n",
    "\n",
    "# Filter the text values out of df but keep null values\n",
    "text_filter = ~df['ASSUMABLEMORTGAGE'].isin(yes_values) | df['ASSUMABLEMORTGAGE'].isNull()\n",
    "df = df.where(text_filter)\n",
    "\n",
    "# Print count of remaining records\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean, stddev\n",
    "\n",
    "# Calculate values used for outlier filtering\n",
    "mean_val = df.agg({'log_SalesClosePrice': 'mean'}).collect()[0][0]\n",
    "stddev_val = df.agg({'log_SalesClosePrice': 'stddev'}).collect()[0][0]\n",
    "\n",
    "# Create three standard deviation (μ ± 3σ) lower and upper bounds for data\n",
    "low_bound = mean_val - (3 * stddev_val)\n",
    "hi_bound = mean_val + (3 * stddev_val)\n",
    "\n",
    "# Filter the data to fit between the lower and upper bounds\n",
    "df = df.where((df['log_SalesClosePrice'] < hi_bound) & (df['log_SalesClosePrice'] > low_bound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define max and min values and collect them\n",
    "max_days = df.agg({'DAYSONMARKET': 'max'}).collect()[0][0]\n",
    "min_days = df.agg({'DAYSONMARKET': 'min'}).collect()[0][0]\n",
    "\n",
    "# Create a new column based off the scaled data\n",
    "df = df.withColumn('percentage_scaled_days', \n",
    "                  round((df['DAYSONMARKET'] - min_days) / (max_days - min_days)) * 100)\n",
    "\n",
    "# Calc max and min for new column\n",
    "print(df.agg({'percentage_scaled_days': 'max'}).collect())\n",
    "print(df.agg({'percentage_scaled_days': 'min'}).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min-max scaler\n",
    "def min_max_scaler(df, cols_to_scale):\n",
    "  # Takes a dataframe and list of columns to minmax scale. Returns a dataframe.\n",
    "  for col in cols_to_scale:\n",
    "    # Define min and max values and collect them\n",
    "    max_days = df.agg({col: 'max'}).collect()[0][0]\n",
    "    min_days = df.agg({col: 'min'}).collect()[0][0]\n",
    "    new_column_name = 'scaled_' + col\n",
    "    # Create a new column based off the scaled data\n",
    "    df = df.withColumn(new_column_name, \n",
    "                      (df[col] - min_days) / (max_days - min_days)) \n",
    "    return df\n",
    "  \n",
    "df = min_max_scaler(df, cols_to_scale)\n",
    "# Show that our data is now between 0 and 1\n",
    "df[['DAYSONMARKET', 'scaled_DAYSONMARKET']].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "+ In the slides we showed how you might use log transforms to fix positively skewed data (data whose distribution is mostly to the left). \n",
    "+ To correct negative skew (data mostly to the right) you need to take an extra step called \"reflecting\" before you can apply the inverse of , written as (1/log) to make the data look more like normal a normal distribution. \n",
    "+ Reflecting data uses the following formula to reflect each value: (Max of X + 1) - x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import log\n",
    "\n",
    "# Compute the skewness\n",
    "print(df.agg({'YEARBUILT': 'skewness'}).collect())\n",
    "\n",
    "# Calculate the max year\n",
    "max_year = df.agg({'YEARBUILT': 'max'}).collect()[0][0]\n",
    "\n",
    "# Create a new column of reflected data\n",
    "df = df.withColumn('Reflect_YearBuilt', (max_year + 1) - df['YEARBUILT'])\n",
    "\n",
    "# Create a new column based reflected data\n",
    "df = df.withColumn('adj_yearbuilt', 1 / log(df['Reflect_YearBuilt']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample the dataframe and convert to Pandas\n",
    "sample_df = df.select(columns).sample(False, 0.1, 42)\n",
    "pandas_df = sample_df.toPandas()\n",
    "\n",
    "# Convert all values to T/F\n",
    "tf_df = pandas_df.isnull()\n",
    "\n",
    "# Plot it\n",
    "sns.heatmap(data=tf_df)\n",
    "plt.xticks(rotation=30, fontsize=10)\n",
    "plt.yticks(rotation=0, fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill na values\n",
    "# Count missing rows\n",
    "missing = df.where(df['PDOM'].isNull()).count()\n",
    "\n",
    "# Calculate the mean value\n",
    "col_mean = df.agg({'PDOM': 'mean'}).collect()[0][0]\n",
    "\n",
    "# Replacing with the mean value for that column\n",
    "df.fillna(col_mean, subset=['PDOM'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping columns where the missing values are greater than a threshold\n",
    "def column_dropper(df, threshold):\n",
    "  # Takes a dataframe and threshold for missing values. Returns a dataframe.\n",
    "  total_records = df.count()\n",
    "  for col in df.columns:\n",
    "    # Calculate the percentage of missing values\n",
    "    missing = df.where(df[col].isNull()).count()\n",
    "    missing_percent = missing / total_records\n",
    "    # Drop column if percent of missing is more than threshold\n",
    "    if missing_percent > threshold:\n",
    "      df = df.drop(col)\n",
    "  return df\n",
    "\n",
    "# Drop columns that are more than 60% missing\n",
    "df = column_dropper(df, .6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe joins\n",
    "DataFrame.join(other_df, on= None, how = None)\n",
    "\n",
    "# Cast data types\n",
    "walk_df = walk_df.withColumn('longitude', walk_df['longitude'].cast('double'))\n",
    "walk_df = walk_df.withColumn('latitude', walk_df['latitude'].cast('double'))\n",
    "\n",
    "# Round precision\n",
    "df = df.withColumn('longitude', round(df['longitude'], 5))\n",
    "df = df.withColumn('latitude', round(df['latitude'], 5))\n",
    "\n",
    "# Create join condition\n",
    "condition = [walk_df['latitude'] == df['latitude'], walk_df['longitude'] == df['longitude']]\n",
    "\n",
    "# Join the dataframes together\n",
    "join_df = df.join(walk_df, on=condition, how='left')\n",
    "# Count non-null records from new field\n",
    "print(join_df.where(~join_df['walkscore'].isNull()).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " Register dataframes as tables\n",
    "df.createOrReplaceTempView('df')\n",
    "walk_df.createOrReplaceTempView('walk_df')\n",
    "\n",
    "# SQL to join dataframes\n",
    "join_sql = \t\"\"\"\n",
    "\t\t\tSELECT \n",
    "\t\t\t\t*\n",
    "\t\t\tFROM df\n",
    "\t\t\tLEFT JOIN walk_df\n",
    "\t\t\tON df.longitude = walk_df.longitude\n",
    "\t\t\tAND df.latitude = walk_df.latitude\n",
    "\t\t\t\"\"\"\n",
    "# Perform sql join\n",
    "joined_df = spark.sql(join_sql)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
