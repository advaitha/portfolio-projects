{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning in PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Schemas\n",
    "+ May contain various data types\n",
    "+ Can filter garbage data during import\n",
    "+ Improves read performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.types\n",
    "peopleSchema = StructType([\n",
    "    StructField('name', StringType(), True), # Boolean is if data can be null or not\n",
    "    StructField('age', IntegerType(), True),\n",
    "    StructField('city',StringType(), True)    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_df = spark.read.format('csv').load(name = 'rawdata.csv', \n",
    "                                          schema = peopleSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pyspark.sql.types library\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Define a new schema using the StructType method\n",
    "people_schema = StructType([\n",
    "  # Define a StructField for each field\n",
    "  StructField('name',StringType(), False),\n",
    "  StructField('age', IntegerType(),False),\n",
    "  StructField('city',StringType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file\n",
    "aa_dfw_df = spark.read.format('csv').options(Header=True).load('AA_DFW_2018.csv.gz')\n",
    "\n",
    "# Add the airport column using the F.lower() method\n",
    "aa_dfw_df = aa_dfw_df.withColumn('airport', F.lower(aa_dfw_df['Destination Airport']))\n",
    "\n",
    "# Drop the Destination Airport column\n",
    "aa_dfw_df = aa_dfw_df.drop(aa_dfw_df['Destination Airport'])\n",
    "\n",
    "# Show the DataFrame\n",
    "aa_dfw_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Difficulties with CSV files\n",
    "+ No Defined schema\n",
    "+ Nested data requires special handling\n",
    "+ Slow to parse\n",
    "+ If schema is not provided, all data must be read before inferring schema\n",
    "+ Predicate pushdown - Ordering tasks to do least amount of work. Filtering the data before processing is one of the optimizations for large datasets. In CSV this cannot be done.\n",
    "+ Any intermediate use requires redefining schemas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parquet Format\n",
    "+ A Columnar data format\n",
    "+ Supported in Spark \n",
    "+ Supports predicate pushdown\n",
    "+ Automatically stores schema information\n",
    "+ Binary file format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading parquet files\n",
    "df = spark.read.format('parquet').load('filename.parquet')\n",
    "df = spark.read.parquet('filename.parquet')\n",
    "df.createOrReplaceTempView('flights')\n",
    "\n",
    "# Writing parquet files\n",
    "df.write.format('parquet').save('filename.parquet')\n",
    "df.write.parquet('filename.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the row count of df1 and df2\n",
    "print(\"df1 Count: %d\" % df1.count())\n",
    "print(\"df2 Count: %d\" % df2.count())\n",
    "\n",
    "# Combine the DataFrames into one\n",
    "df3 = df1.union(df2)\n",
    "\n",
    "# Save the df3 DataFrame in Parquet format\n",
    "df3.write.parquet('AA_DFW_ALL.parquet', mode='overwrite')\n",
    "\n",
    "# Read the Parquet file into a new DataFrame and run a count\n",
    "print(spark.read.parquet('AA_DFW_ALL.parquet').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Parquet file into flights_df\n",
    "flights_df = spark.read.parquet('AA_DFW_ALL.parquet')\n",
    "\n",
    "# Register the temp table\n",
    "flights_df.createOrReplaceTempView('flights')\n",
    "\n",
    "# Run a SQL query of the average flight duration\n",
    "avg_duration = spark.sql('SELECT avg(flight_duration) from flights').collect()[0]\n",
    "print('The average flight time is: %d' % avg_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
